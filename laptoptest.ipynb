{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebc9f4-1e8d-46b0-b68f-12cda41c1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install adversarial-robustness-toolbox\n",
    "!pip install torch\n",
    "!pip install pytorchyolo\n",
    "!pip install pillow\n",
    "!pip install numpy --upgrade\n",
    "!pip install yolov5\n",
    "!pip install --upgrad matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f59977-563a-4fd0-b241-7db8d1978daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.object_detection.pytorch_detection_transformer import PyTorchDetectionTransformer\n",
    "from art.attacks.evasion.adversarial_patch.adversarial_patch_pytorch import AdversarialPatchPyTorch\n",
    "from art.estimators.object_detection.pytorch_yolo import PyTorchYolo\n",
    "\n",
    "from art.attacks.evasion import RobustDPatch\n",
    "from torchvision.transforms import transforms\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import yolov5\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if gpu is available:\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.current_device()\n",
    "\n",
    "\n",
    "torch.cuda.device(0)\n",
    "\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fecb1a-f5b1-497f-8a2a-598aa7a3a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \n",
    "        'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d09e89-51bb-4a0f-a692-0608017736d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"vvcccvc\"\n",
    "dataset_dir=\"./coco_resized/validation/\"\n",
    "dataset_type = fo.types.COCODetectionDataset\n",
    "dataset = fo.Dataset.from_dir(dataset_dir, dataset_type, name=name)\n",
    "dataset.persistent = False\n",
    "#predictions_view = dataset.take(2, seed=51)\n",
    "predictions_view = dataset.take(100, seed=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f454e1f-ffe1-49ec-a725-8de1964dacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(predictions, conf_thresh):\n",
    "    dictionary = {}\n",
    "\n",
    "    boxes_list = []\n",
    "    scores_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for i in range(len(predictions[0][\"boxes\"])):\n",
    "        score = predictions[0][\"scores\"][i]\n",
    "        if score >= conf_thresh:\n",
    "            boxes_list.append(predictions[0][\"boxes\"][i])\n",
    "            scores_list.append(predictions[0][\"scores\"][[i]])\n",
    "            labels_list.append(predictions[0][\"labels\"][[i]])\n",
    "    dictionary[\"boxes\"] = np.vstack(boxes_list)\n",
    "    dictionary[\"scores\"] = np.hstack(scores_list)\n",
    "    dictionary[\"labels\"] = np.hstack(labels_list)\n",
    "\n",
    "    y = [dictionary]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictions(predictions_, conf_thresh):\n",
    "    # Get the predicted class\n",
    "    predictions_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(predictions_[\"labels\"])]\n",
    "    #  print(\"\\npredicted classes:\", predictions_class)\n",
    "    if len(predictions_class) < 1:\n",
    "        return [], [], []\n",
    "    # Get the predicted bounding boxes\n",
    "    predictions_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(predictions_[\"boxes\"])]\n",
    "\n",
    "    # Get the predicted prediction score\n",
    "    predictions_score = list(predictions_[\"scores\"])\n",
    "    # print(\"predicted score:\", predictions_score)\n",
    "\n",
    "    # Get a list of index with score greater than threshold\n",
    "    threshold = conf_thresh\n",
    "    predictions_t = [predictions_score.index(x) for x in predictions_score if x > threshold]\n",
    "    if len(predictions_t) > 0:\n",
    "        predictions_t = predictions_t  # [-1] #indices where score over threshold\n",
    "    else:\n",
    "        # no predictions esxceeding threshold\n",
    "        return [], [], []\n",
    "    # predictions in score order\n",
    "    predictions_boxes = [predictions_boxes[i] for i in predictions_t]\n",
    "    predictions_class = [predictions_class[i] for i in predictions_t]\n",
    "    predictions_scores = [predictions_score[i] for i in predictions_t]\n",
    "    return predictions_class, predictions_boxes, predictions_scores\n",
    "\n",
    "\n",
    "def plot_image_with_boxes(img, boxes, pred_cls, title):\n",
    "    text_size = 1\n",
    "    text_th = 3\n",
    "    rect_th = 2\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        cv2.rectangle(img, (int(boxes[i][0][0]), int(boxes[i][0][1])), (int(boxes[i][1][0]), int(boxes[i][1][1])),\n",
    "                      color=(0, 255, 0), thickness=rect_th)\n",
    "        # Write the prediction class\n",
    "        cv2.putText(img, pred_cls[i], (int(boxes[i][0][0]), int(boxes[i][0][1])), cv2.FONT_HERSHEY_SIMPLEX, text_size,\n",
    "                    (0, 255, 0), thickness=text_th)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(img.astype(np.uint8), interpolation=\"nearest\")\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c6344-bcb4-4698-862f-03d422156a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "NUMBER_CHANNELS = 3\n",
    "INPUT_SHAPE = (3, 800, 800)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(INPUT_SHAPE[1], interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(INPUT_SHAPE[1]),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0bf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'yolov5' # OR yolov5\n",
    "\n",
    "\n",
    "if MODEL == 'yolov3':\n",
    "\n",
    "    from pytorchyolo.utils.loss import compute_loss\n",
    "    from pytorchyolo.models import load_model\n",
    "\n",
    "    class Yolo(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x, targets=None):\n",
    "            if self.training:\n",
    "                outputs = self.model(x)\n",
    "                loss, loss_components = compute_loss(outputs, targets, self.model)\n",
    "                loss_components_dict = {\"loss_total\": loss}\n",
    "                loss_components_dict['loss_box'] = loss_components[0]\n",
    "                loss_components_dict['loss_obj'] = loss_components[1]\n",
    "                loss_components_dict['loss_cls'] = loss_components[2]\n",
    "                return loss_components_dict\n",
    "            else:\n",
    "                tmp = self.model(x)\n",
    "                return tmp\n",
    "\n",
    "    model_path = \"./../../yolov3.cfg\"\n",
    "    weights_path = \"./../../yolov3.weights\"\n",
    "    model = load_model(model_path=model_path, weights_path=weights_path)\n",
    "            \n",
    "    model = Yolo(model)\n",
    "\n",
    "    detector = PyTorchYolo(model=model,\n",
    "                        device_type='cpu',\n",
    "                        input_shape=(3, 640, 640),\n",
    "                        clip_values=(0, 255), \n",
    "                        attack_losses=(\"loss_total\", \"loss_cls\",\n",
    "                                        \"loss_box\",\n",
    "                                        \"loss_obj\"))\n",
    "\n",
    "elif MODEL == 'yolov5':\n",
    "\n",
    "    import yolov5\n",
    "    from yolov5.utils.loss import ComputeLoss\n",
    "\n",
    "    class Yolo(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            self.model.hyp = {'box': 0.05,\n",
    "                            'obj': 1.0,\n",
    "                            'cls': 0.5,\n",
    "                            'anchor_t': 4.0,\n",
    "                            'cls_pw': 1.0,\n",
    "                            'obj_pw': 1.0,\n",
    "                            'fl_gamma': 0.0\n",
    "                            }\n",
    "            self.compute_loss = ComputeLoss(self.model.model.model)\n",
    "\n",
    "        def forward(self, x, targets=None):\n",
    "            if self.training:\n",
    "                outputs = self.model.model.model(x)\n",
    "                loss, loss_items = self.compute_loss(outputs, targets)\n",
    "                loss_components_dict = {\"loss_total\": loss}\n",
    "                loss_components_dict['loss_box'] = loss_items[0]\n",
    "                loss_components_dict['loss_obj'] = loss_items[1]\n",
    "                loss_components_dict['loss_cls'] = loss_items[2]\n",
    "                return loss_components_dict\n",
    "            else:\n",
    "                return self.model(x)\n",
    "\n",
    "    model = yolov5.load('yolov5s.pt')\n",
    "    \n",
    "    model = Yolo(model)\n",
    "\n",
    "    detector = PyTorchYolo(model=model,\n",
    "                        device_type='gpu',\n",
    "                        clip_values=(0, 255), \n",
    "                        input_shape=(3, 800, 800),\n",
    "                        attack_losses=(\"loss_total\", \"loss_cls\",\n",
    "                                        \"loss_box\",\n",
    "                                        \"loss_obj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b2095-adfd-4263-86e1-698e72a9cf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40241343-c417-49d8-bc9f-df19ccc95897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbers(filename):\n",
    "   numbers = re.findall(r'\\d+', filename)\n",
    "   return ''.join(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba09d23-24f5-4da6-a101-af84df282627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a2fd5-6f3e-4965-83d4-246961d85f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images = []\n",
    "image_ids = []\n",
    "image_sizes = []\n",
    "unsized_images = []\n",
    "import PIL\n",
    "import os\n",
    "for sample in predictions_view:\n",
    "    image_id = os.path.basename(sample.filepath)\n",
    "    image_id = image_id.replace('.jpg', '')\n",
    "    image_ids.append(image_id)\n",
    "    \n",
    "    im = PIL.Image.open(sample.filepath)\n",
    "    \n",
    "    \n",
    "    im = im.convert('RGB')\n",
    "\n",
    "    image_sizes.append(im.size)\n",
    "    im = transform(im).numpy()\n",
    "    \n",
    "    coco_images.append(im)\n",
    "coco_images = np.array(coco_images) * 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dets = detector.predict(coco_images) \n",
    "for i in range(len(dets)):\n",
    "    preds_orig = extract_predictions(dets[i], 0.5)\n",
    "    if i%50 ==0:\n",
    "        plot_image_with_boxes(img=coco_images[i].transpose(1,2,0).copy(), boxes=preds_orig[1], pred_cls=preds_orig[0], title=\"Predictions on image without patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffc21b-2be2-46f9-8be5-801a49d16d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from art.attacks.evasion import DPatch\n",
    "print(\"Starting dpatch\")\n",
    "x = coco_images[:-1]\n",
    "\n",
    "dets = detector.predict(x)\n",
    "patch_shape = (3, 80, 80)\n",
    "\n",
    "attack = RobustDPatch(\n",
    "    detector,\n",
    "    patch_shape=patch_shape,\n",
    "    patch_location=(50, 50),\n",
    "    crop_range=[0,0],\n",
    "    brightness_range=[1.0, 1.0],\n",
    "    rotation_weights=[1, 0, 0, 0],\n",
    "    sample_size=1,\n",
    "    learning_rate=1.99,\n",
    "    max_iter=1,\n",
    "    batch_size=16,\n",
    "    verbose=False,\n",
    "    targeted=False\n",
    ")\n",
    "patch_shape = tuple(filter(lambda x: x != 3, patch_shape))\n",
    "\n",
    "# Convert tuple to string\n",
    "patch_shape = ''.join(map(str, patch_shape))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for i in tqdm(range(500)):\n",
    "   patch = attack.generate(x)\n",
    "   patched_images = attack.apply_patch(x)\n",
    "\n",
    "_y = detector.predict(patched_images)\n",
    "\n",
    "\n",
    "for i in range(len(_y)):\n",
    "    preds = extract_predictions(_y[i], 0.5)\n",
    "    # Otetaan bbox talteen:\n",
    "    boxes = preds[1]\n",
    "    labels = preds[0]\n",
    "    scores = preds[2]\n",
    "    detections = []\n",
    "    for j, count in enumerate(range(len(boxes))):      \n",
    "       # And the image dimensions are known\n",
    "       image_width = 800\n",
    "       image_height = 800\n",
    "       normalized_bbox = [\n",
    "       boxes[j][0][0] / image_width, # xmin\n",
    "       boxes[j][0][1] / image_height, # ymin\n",
    "       boxes[j][1][0] / image_width, # xmax\n",
    "       boxes[j][1][1] / image_height  # ymax\n",
    "       ]\n",
    "       # Modified code to convert to (x0, y0, w, h) format\n",
    "       x0 = normalized_bbox[0]\n",
    "       y0 = normalized_bbox[1]\n",
    "       x1 = normalized_bbox[2]\n",
    "       y1 = normalized_bbox[3]\n",
    "    \n",
    "       w = x1 - x0\n",
    "       h = y1 - y0\n",
    "       new_bbox = [x0, y0, w, h]\n",
    "\n",
    "       tensors = [torch.tensor(value) for value in new_bbox]\n",
    "       tensors = [tensor.float() for tensor in tensors]\n",
    "       \n",
    "       scalar_value = scores[j]\n",
    "\n",
    "        # Convert to tensor\n",
    "       tensor_value = torch.tensor(scalar_value)\n",
    "       score=tensor_value\n",
    "       detections.append(\n",
    "           fo.Detection(\n",
    "               label=labels[j],\n",
    "               bounding_box=tensors,\n",
    "               confidence=score\n",
    "           )\n",
    "       )\n",
    "    plot_image_with_boxes(img=patched_images[i].transpose(1,2,0).copy(), boxes=preds[1], pred_cls=preds[0], title=\"Predictions on image without patch\")\n",
    "    sample = dataset[f\"/scratch/project_2008539/coco_resized/validation/data/{image_ids[i]}.jpg\"]\n",
    "    sample[\"yolov5\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n",
    "    # ongelma: sampleja ei käydä läpi joten tätä tietoa ei myöskään missään vaiheessa tallenneta evaluaatiota varten.\n",
    "print(detections)\n",
    "#print(sample)\n",
    "print(\"finished adding predictions\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = fo.launch_app(dataset)\n",
    "#session.view = predictions_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb56e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf_view = predictions_view.filter_labels(\"yolov5\", F(\"confidence\") > 0.75, only_matches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_conf_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9143176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a prediction from the view to verify that its confidence is > 0.75\n",
    "sample = high_conf_view.first()\n",
    "print(sample.yolov5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46db5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions in the `yolo` field of our `high_conf_view`\n",
    "# with respect to the objects in the `ground_truth` field\n",
    "results = predictions_view.evaluate_detections(\n",
    "    \"yolov5\",\n",
    "    gt_field=\"detections\",\n",
    "    eval_key=\"yolo_eval\",\n",
    "    compute_mAP=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b51099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10 most common classes in the dataset\n",
    "counts = dataset.count_values(\"detections.detections.label\")\n",
    "classes_top10 = sorted(counts, key=counts.get, reverse=True)[:10]\n",
    "\n",
    "# Print a classification report for the top-10 classes\n",
    "results.print_report(classes=classes_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.1547309528465194 käytännössä ilman kouluttamista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe34771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
